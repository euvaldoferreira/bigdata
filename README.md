# ğŸš€ Ambiente BigData Integrado

Este projeto fornece um ambiente completo de Big Data usando Docker, integrando Airflow, MinIO, Jenkins, PySpark e Jupyter Notebook para desenvolvimento e processamento de dados em larga escala.

## ğŸ“‹ VisÃ£o Geral

O ambiente inclui os seguintes serviÃ§os integrados:

- **ğŸŒŠ Apache Airflow 2.7.0**: OrquestraÃ§Ã£o de workflows e pipelines de dados
- **ğŸ“¦ MinIO**: Storage de objetos compatÃ­vel com S3
- **ğŸ”§ Jenkins**: CI/CD para automaÃ§Ã£o de builds e deploys
- **âš¡ Apache Spark 3.4.0**: Processamento distribuÃ­do de dados
- **ğŸ““ Jupyter Notebook**: Desenvolvimento interativo e anÃ¡lise de dados (PySpark)
- **ğŸ˜ PostgreSQL 13**: Banco de dados para metadados do Airflow
- **ğŸ“® Redis**: Message broker para Celery (Airflow)

## ï¿½ ConfiguraÃ§Ãµes DisponÃ­veis

### ğŸ”¬ **Ambiente MÃ­nimo** (`make minimal`)
- **Recursos:** 3-4GB RAM, 2 CPUs, 5-8GB disco
- **ServiÃ§os:** Airflow Standalone + Jupyter/Spark Local + MinIO + PostgreSQL
- **Ideal para:** Desenvolvimento bÃ¡sico, aprendizado, mÃ¡quinas com poucos recursos

### ğŸ§ª **Ambiente Lab** (`make lab`) 
- **Recursos:** 6-8GB RAM, 4 CPUs, 10-15GB disco
- **ServiÃ§os:** Airflow + Spark Cluster + MinIO + Jupyter + Redis
- **Ideal para:** LaboratÃ³rio, testes, desenvolvimento intermediÃ¡rio

### ğŸ­ **Ambiente Completo** (`make start`)
- **Recursos:** 10-12GB RAM, 6-8 cores, 20GB+ disco
- **ServiÃ§os:** Todos os serviÃ§os incluindo Jenkins e Workers distribuÃ­dos
- **Ideal para:** ProduÃ§Ã£o, desenvolvimento avanÃ§ado

ğŸ’¡ **Use `make requirements` para ver detalhes completos dos recursos.**

ğŸ’¡ **Use `make requirements` para ver detalhes completos dos recursos.**

## âš™ï¸ ConfiguraÃ§Ã£o

### ğŸ“ Arquivo de VariÃ¡veis de Ambiente

Antes de iniciar o ambiente, configure as variÃ¡veis de ambiente:

```bash
# Copie o arquivo de exemplo
cp .env.example .env

# Edite com suas configuraÃ§Ãµes
nano .env
```

**Principais configuraÃ§Ãµes no arquivo `.env`:**

```bash
# IP do servidor (para acesso externo)
SERVER_IP=192.168.1.22

# Senhas personalizadas (ALTERE!)
POSTGRES_PASSWORD=sua_senha_postgres_aqui
AIRFLOW_ADMIN_PASSWORD=sua_senha_airflow_aqui
MINIO_ROOT_PASSWORD=sua_senha_minio_aqui

# Portas (opcional - sÃ³ mude se houver conflito)
AIRFLOW_PORT=8080
JUPYTER_PORT=8888
MINIO_CONSOLE_PORT=9001
```

âš ï¸ **Importante:** Sempre altere as senhas padrÃ£o em ambientes de produÃ§Ã£o!

### ğŸŒ Descobrindo o IP do Servidor

Antes de configurar o `.env`, vocÃª precisa saber o IP do seu servidor:

#### OpÃ§Ã£o A: DetecÃ§Ã£o AutomÃ¡tica (Recomendado)
```bash
# Descobre todos os IPs disponÃ­veis
make get-ip

# Detecta e configura automaticamente o IP principal
make auto-ip
```

#### OpÃ§Ã£o B: Comandos Manuais do Sistema
```bash
# MÃ©todo 1: IP da interface principal (mais usado)
ip route get 8.8.8.8 | awk '{print $7; exit}'

# MÃ©todo 2: Primeiro IP nÃ£o-localhost
hostname -I | awk '{print $1}'

# MÃ©todo 3: Listar todas as interfaces
ip addr show | grep -E 'inet [0-9]' | grep -v '127.0.0.1'
```

#### OpÃ§Ã£o C: ConfiguraÃ§Ã£o Manual
```bash
# Configurar IP especÃ­fico
make set-ip IP=192.168.1.22

# Ou editar diretamente o .env
nano .env  # Altere SERVER_IP=SEU_IP_AQUI
```

**ğŸ’¡ Dicas:**
- Para acesso local apenas: use `SERVER_IP=localhost` ou `SERVER_IP=127.0.0.1`
- Para acesso de outras mÃ¡quinas: use o IP da rede (ex: `192.168.1.22`)
- Em VPS/Cloud: use o IP pÃºblico ou privado conforme necessÃ¡rio

## ğŸš€ InstalaÃ§Ã£o e Uso

### 1. Clone ou baixe o projeto

```bash
git clone <seu-repositorio>
cd containers
```

### 2. Verifique se o servidor estÃ¡ pronto

```bash
# VerificaÃ§Ã£o rÃ¡pida dos requisitos mÃ­nimos
make pre-check

# VerificaÃ§Ã£o completa do sistema (recomendado)
make check
```

O comando `make check` verifica:
- âœ… Docker e Docker Compose instalados
- âœ… Recursos do sistema (RAM, CPU, disco)
- âœ… ConfiguraÃ§Ãµes vÃ¡lidas
- âœ… Docker daemon rodando
- ğŸ’¡ Recomenda o ambiente ideal para seu hardware

### 3. Configure o ambiente

```bash
# OpÃ§Ã£o A: ConfiguraÃ§Ã£o automÃ¡tica (recomendado)
make auto-ip          # Detecta e configura IP automaticamente
cp .env.example .env  # Se ainda nÃ£o existe
nano .env            # Ajuste senhas e outras configuraÃ§Ãµes

# OpÃ§Ã£o B: ConfiguraÃ§Ã£o manual
cp .env.example .env
make get-ip          # Veja IPs disponÃ­veis
nano .env           # Configure SERVER_IP e senhas manualmente
```

### 4. Inicie o ambiente

#### OpÃ§Ã£o A: Usando Makefile (Recomendado)
```bash
# Ver todos os comandos disponÃ­veis
make help

# Ver requisitos de sistema
make requirements

# Escolher configuraÃ§Ã£o baseada nos seus recursos:
make minimal     # Para 3-4GB RAM (bÃ¡sico)
make lab         # Para 6-8GB RAM (laboratÃ³rio)  
make start       # Para 10GB+ RAM (completo)

# Verificar status
make status
```

#### OpÃ§Ã£o B: Usando scripts diretamente
```bash
./scripts/start.sh
```

O sistema irÃ¡:
- Verificar dependÃªncias
- Criar diretÃ³rios necessÃ¡rios
- Configurar variÃ¡veis de ambiente
- Inicializar todos os serviÃ§os em ordem
- Aguardar que todos estejam prontos

### 3. Acesse os serviÃ§os

ApÃ³s a inicializaÃ§Ã£o completa, os serviÃ§os estarÃ£o disponÃ­veis em:

| ServiÃ§o | URL | Credenciais |
|---------|-----|-------------|
| **Airflow** | http://SERVER_IP:8080 | admin/admin |
| **Spark Master UI** | http://SERVER_IP:8081 | - |
| **MinIO Console** | http://SERVER_IP:9001 | minioadmin/minioadmin123 |
| **Jenkins** | http://SERVER_IP:8082 | admin/admin |
| **Jupyter Notebook** | http://SERVER_IP:8888 | sem senha |
| **Flower (Celery)** | http://SERVER_IP:5555 | - |

ğŸ“ **Nota:** Substitua `SERVER_IP` pelo IP configurado no arquivo `.env` (exemplo: 192.168.1.22 ou localhost)

ğŸ’¡ **Dica:** Use `make ports` para ver as URLs exatas configuradas no seu ambiente.

## ğŸ“ Estrutura do Projeto

```
containers/
â”œâ”€â”€ docker-compose.yml          # ConfiguraÃ§Ã£o completa (produÃ§Ã£o)
â”œâ”€â”€ docker-compose.minimal.yml  # ConfiguraÃ§Ã£o mÃ­nima (4GB RAM)
â”œâ”€â”€ docker-compose.lab.yml      # ConfiguraÃ§Ã£o laboratÃ³rio (6GB RAM)
â”œâ”€â”€ .env                       # VariÃ¡veis de ambiente (configurado pelo usuÃ¡rio)
â”œâ”€â”€ .env.example                # Template de variÃ¡veis de ambiente
â”œâ”€â”€ Makefile                   # AutomaÃ§Ã£o de comandos
â”œâ”€â”€ README.md                  # Este arquivo
â”œâ”€â”€ PARAMETRIZACAO.md          # DocumentaÃ§Ã£o de parametrizaÃ§Ã£o
â”œâ”€â”€ airflow/                   # ConfiguraÃ§Ãµes do Airflow
â”‚   â”œâ”€â”€ dags/                 # DAGs do Airflow
â”‚   â”œâ”€â”€ plugins/              # Plugins customizados
â”‚   â”œâ”€â”€ config/               # ConfiguraÃ§Ãµes
â”‚   â””â”€â”€ requirements.txt      # DependÃªncias Python
â”œâ”€â”€ jenkins/                   # ConfiguraÃ§Ãµes do Jenkins
â”‚   â”œâ”€â”€ Dockerfile           # Imagem customizada
â”‚   â”œâ”€â”€ plugins.txt          # Lista de plugins
â”‚   â””â”€â”€ init.groovy.d/       # Scripts de inicializaÃ§Ã£o
â”œâ”€â”€ spark/                     # ConfiguraÃ§Ãµes do Spark
â”‚   â”œâ”€â”€ apps/                # AplicaÃ§Ãµes Spark
â”‚   â”œâ”€â”€ conf/                # ConfiguraÃ§Ãµes
â”‚   â””â”€â”€ data/                # Dados do Spark
â”œâ”€â”€ jupyter/                   # ConfiguraÃ§Ãµes do Jupyter
â”‚   â”œâ”€â”€ notebooks/           # Notebooks Jupyter
â”‚   â””â”€â”€ config/              # ConfiguraÃ§Ãµes
â”œâ”€â”€ scripts/                   # Scripts utilitÃ¡rios
â”‚   â”œâ”€â”€ start.sh             # Inicializar ambiente
â”‚   â”œâ”€â”€ stop.sh              # Parar ambiente
â”‚   â”œâ”€â”€ status.sh            # Status dos serviÃ§os
â”‚   â””â”€â”€ setup-minio.sh       # ConfiguraÃ§Ã£o do MinIO
â”œâ”€â”€ Makefile                   # AutomaÃ§Ã£o de comandos
â”œâ”€â”€ data/                      # Dados compartilhados
â””â”€â”€ logs/                      # Logs dos serviÃ§os
```

## ğŸ› ï¸ Comandos Ãšteis

### ğŸ“‹ Comandos Principais

```bash
# Ver todos os comandos disponÃ­veis
make help

# Iniciar ambientes
make minimal            # Ambiente mÃ­nimo (4GB RAM)
make lab               # Ambiente laboratÃ³rio (6GB RAM)  
make start             # Ambiente completo (10GB+ RAM)

# Parar ambientes
make stop              # Para o ambiente atual
make stop-minimal      # Para especificamente o ambiente mÃ­nimo
make stop-lab          # Para especificamente o ambiente lab
make stop-all          # Para TODOS os containers de todos os ambientes

# Status e monitoramento
make health            # Verifica saÃºde de todos os serviÃ§os
make status            # Status detalhado via script
make ps                # Lista containers do ambiente atual
make ps-all            # Lista containers de TODOS os ambientes
make ports             # Mostra portas e URLs de acesso
```

### ğŸ“Š Monitoramento e Logs

```bash
# Logs gerais
make logs              # Ver logs de todos os serviÃ§os
make logs-airflow      # Logs especÃ­ficos do Airflow
make logs-spark        # Logs especÃ­ficos do Spark
make logs-jupyter      # Logs do Jupyter
make logs-minio        # Logs do MinIO
make logs-jenkins      # Logs do Jenkins

# Recursos
make top               # Uso de recursos dos containers
make info              # InformaÃ§Ãµes do ambiente
make version           # VersÃµes dos componentes
```

### ğŸ”§ Desenvolvimento

```bash
# Acessar shells dos serviÃ§os
make airflow-shell     # Acessar shell do Airflow
make spark-shell       # Acessar Spark shell
make jupyter-shell     # Acessar shell do Jupyter
make minio-shell       # Acessar shell do MinIO

# OperaÃ§Ãµes com Spark
make submit-spark      # Submeter job Spark de exemplo

# Reiniciar serviÃ§os especÃ­ficos
make restart-airflow   # Reinicia apenas o Airflow
make restart-spark     # Reinicia cluster Spark
make restart-jupyter   # Reinicia Jupyter
```

### ğŸ§ª Testes e ValidaÃ§Ã£o

```bash
# Testes de integraÃ§Ã£o
make test              # Executa todos os testes
make test-airflow      # Testa Airflow
make test-spark        # Testa cluster Spark
make test-minio        # Testa MinIO

# VerificaÃ§Ãµes
make check             # Verifica configuraÃ§Ã£o e dependÃªncias
make requirements      # Mostra requisitos de sistema
```

### ğŸ—‚ï¸ ManutenÃ§Ã£o e Dados

```bash
# Backup e restore
make backup            # Fazer backup dos dados
make clean             # Limpar dados (CUIDADO!)
make clean-images      # Remove imagens nÃ£o utilizadas

# AtualizaÃ§Ãµes
make pull              # Atualiza todas as imagens
make rebuild           # Rebuild completo (pull + build)
```

### ğŸŒ Comandos de Rede

```bash
# Portas e acesso
make ports             # Lista portas utilizadas
make open              # Abre todas as interfaces web (Linux)

# Exemplo de saÃ­da do make ports:
# â€¢ Airflow:     http://${SERVER_IP}:8080
# â€¢ Spark UI:    http://${SERVER_IP}:8081  
# â€¢ MinIO:       http://${SERVER_IP}:9001
# â€¢ Jupyter:     http://${SERVER_IP}:8888
```
```

### Gerenciamento do Ambiente (Scripts)

```bash
# Iniciar ambiente completo
./scripts/start.sh

# Verificar status
./scripts/status.sh

# Parar ambiente
./scripts/stop.sh

# Parar e limpar todos os dados
./scripts/stop.sh --clean
```

## ğŸ¯ Comandos EspecÃ­ficos por ServiÃ§o

O ambiente oferece comandos granulares para gerenciar cada serviÃ§o individualmente:

### ğŸ˜ PostgreSQL
```bash
# Gerenciamento bÃ¡sico
make start-postgres    # Inicia apenas PostgreSQL
make stop-postgres     # Para PostgreSQL  
make restart-postgres  # Reinicia PostgreSQL
make health-postgres   # Verifica saÃºde
make logs-postgres     # Visualiza logs

# Acesso e debug
make shell-postgres    # Conecta via psql
make debug-postgres    # Debug completo
make reset-postgres    # Reset completo (REMOVE DADOS!)
make backup-postgres   # Backup dos dados
```

### ğŸ“® Redis  
```bash
# Gerenciamento bÃ¡sico
make start-redis       # Inicia apenas Redis
make stop-redis        # Para Redis
make restart-redis     # Reinicia Redis  
make health-redis      # Verifica saÃºde
make logs-redis        # Visualiza logs

# Acesso
make shell-redis       # Conecta via redis-cli
```

### ğŸ—‚ï¸ MinIO
```bash
# Gerenciamento bÃ¡sico
make start-minio       # Inicia apenas MinIO
make stop-minio        # Para MinIO
make restart-minio     # Reinicia MinIO
make health-minio      # Verifica saÃºde  
make logs-minio        # Visualiza logs

# Acesso e manutenÃ§Ã£o
make shell-minio       # Shell no container
make open-minio        # Abre interface web
make debug-minio       # Debug completo
make backup-minio      # Backup dos dados
make reset-minio       # Reset completo (REMOVE DADOS!)
```

### âœˆï¸ Airflow
```bash
# Gerenciamento bÃ¡sico (inclui dependÃªncias)
make start-airflow     # Inicia Airflow + PostgreSQL + Redis
make stop-airflow      # Para todos os serviÃ§os Airflow  
make restart-airflow   # Reinicia Airflow
make health-airflow    # Verifica saÃºde de todos os componentes

# Logs especÃ­ficos
make logs-airflow-webserver    # Logs do webserver
make logs-airflow-scheduler    # Logs do scheduler  
make logs-airflow-worker       # Logs do worker

# Acesso e manutenÃ§Ã£o
make shell-airflow     # Shell no container
make open-airflow      # Abre interface web
make debug-airflow     # Debug completo
make backup-airflow    # Backup configuraÃ§Ãµes e logs
make check-deps-airflow # Verifica dependÃªncias
```

### âš¡ Spark
```bash
# Gerenciamento bÃ¡sico
make start-spark       # Inicia cluster Spark completo
make stop-spark        # Para cluster Spark
make restart-spark     # Reinicia cluster
make health-spark      # Verifica saÃºde do cluster

# Logs especÃ­ficos  
make logs-spark-master   # Logs do master
make logs-spark-worker   # Logs dos workers

# Acesso e manutenÃ§Ã£o
make shell-spark       # Shell no Spark Master
make open-spark        # Abre Spark UI
make debug-spark       # Debug completo
make check-deps-spark  # Verifica dependÃªncias
```

### ğŸ““ Jupyter
```bash
# Gerenciamento bÃ¡sico
make start-jupyter     # Inicia Jupyter
make stop-jupyter      # Para Jupyter
make restart-jupyter   # Reinicia Jupyter
make health-jupyter    # Verifica saÃºde
make logs-jupyter      # Visualiza logs

# Acesso
make shell-jupyter     # Shell no container
make open-jupyter      # Abre interface web (senha: jupyter)
```

### ğŸ—ï¸ Jenkins
```bash
# Gerenciamento bÃ¡sico
make start-jenkins     # Inicia Jenkins
make stop-jenkins      # Para Jenkins  
make restart-jenkins   # Reinicia Jenkins
make health-jenkins    # Verifica saÃºde
make logs-jenkins      # Visualiza logs

# Acesso
make shell-jenkins     # Shell no container
make open-jenkins      # Abre interface web
```

## ğŸ”§ Comandos de Status Consolidado

```bash
# Status por categoria
make status-database      # PostgreSQL + Redis
make status-storage       # MinIO
make status-compute       # Spark + Jupyter  
make status-orchestration # Airflow + Jenkins

# VerificaÃ§Ã£o de dependÃªncias
make check-deps-airflow   # DependÃªncias do Airflow
make check-deps-spark     # DependÃªncias do Spark
```

## ğŸ’¾ Backup e Restore por ServiÃ§o

```bash
# Backups individuais
make backup-postgres      # Backup PostgreSQL
make backup-minio         # Backup MinIO
make backup-airflow       # Backup Airflow

# Backups completos
make full-backup-postgres # Dados + volume PostgreSQL
make full-backup-minio    # Backup completo MinIO
make full-backup-airflow  # ConfiguraÃ§Ãµes + logs Airflow

# Backup de todo ambiente
make backup-all           # Backup completo de tudo

# Gerenciamento de backups
make list-backups         # Lista todos os backups
make clean-old-backups    # Remove backups >7 dias
```

## ğŸš¨ Troubleshooting por ServiÃ§o

```bash
# Debug especÃ­fico
make debug-postgres       # Debug PostgreSQL completo
make debug-minio          # Debug MinIO completo  
make debug-airflow        # Debug Airflow completo
make debug-spark          # Debug Spark completo

# Reset de serviÃ§os (CUIDADO!)
make reset-postgres       # Remove todos os dados PostgreSQL
make reset-minio          # Remove todos os dados MinIO
make reset-airflow        # Remove logs e configuraÃ§Ãµes

# Rebuild de containers
make rebuild-postgres     # Rebuild PostgreSQL
make rebuild-minio        # Rebuild MinIO
make rebuild-airflow      # Rebuild Airflow
make rebuild-spark        # Rebuild Spark  
make rebuild-jupyter      # Rebuild Jupyter
```

### Docker Compose (Comandos Manuais)

```bash
# Ver logs de um serviÃ§o especÃ­fico
docker-compose logs -f airflow-webserver

# Reiniciar um serviÃ§o
docker-compose restart spark-master

# Acessar container
docker-compose exec jupyter bash

# Ver status dos containers
docker-compose ps
```

### Spark

```bash
# Submeter job Spark (usando variÃ¡veis parametrizadas)
docker-compose exec spark-master spark-submit \
    --master ${SPARK_MASTER_URL} \
    ${SPARK_APPS_PATH}/example_spark_minio.py

# Acessar Spark shell
docker-compose exec spark-master spark-shell \
    --master ${SPARK_MASTER_URL}

# Alternativa com valores fixos (se preferir)
# docker-compose exec spark-master spark-submit \
#     --master spark://spark-master:7077 \
#     /opt/bitnami/spark/apps/example_spark_minio.py
```

### MinIO

```bash
# Listar buckets
docker-compose exec minio mc ls myminio

# Copiar arquivo para bucket
docker-compose exec minio mc cp /data/arquivo.csv myminio/data-lake/
```

## ğŸ“Š Exemplos de Uso

### 1. DAG do Airflow

Um DAG de exemplo estÃ¡ incluÃ­do em `airflow/dags/example_bigdata_integration.py` que demonstra:
- Teste de conexÃ£o com MinIO
- ExecuÃ§Ã£o de job Spark
- GeraÃ§Ã£o de relatÃ³rios

### 2. Notebook Jupyter

Um notebook de exemplo estÃ¡ incluÃ­do em `jupyter/notebooks/BigData_Integration_Example.ipynb` que demonstra:
- ConexÃ£o com Spark
- Processamento de dados
- Salvamento no MinIO
- VisualizaÃ§Ãµes

### 3. Pipeline Jenkins

Um pipeline de exemplo Ã© criado automaticamente que demonstra:
- Build e teste de aplicaÃ§Ãµes
- Deploy para Spark
- Backup no MinIO

## ğŸ”„ Fluxo de Desenvolvimento

1. **Desenvolvimento**: Use Jupyter para exploraÃ§Ã£o e prototipagem
2. **OrquestraÃ§Ã£o**: Crie DAGs no Airflow para workflows automatizados
3. **Processamento**: Desenvolva aplicaÃ§Ãµes Spark para processamento em larga escala
4. **CI/CD**: Use Jenkins para automaÃ§Ã£o de builds e deploys
5. **Storage**: Armazene dados e resultados no MinIO

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Escalabilidade do Spark

Para adicionar mais workers Spark, edite o `docker-compose.yml`:

```yaml
spark-worker-3:
  image: bitnami/spark:3.4.0
  container_name: bigdata_spark_worker_3
  # ... configuraÃ§Ãµes similares aos outros workers
```

### ConfiguraÃ§Ãµes do Airflow

Personalize configuraÃ§Ãµes em `airflow/config/airflow.cfg`:
- Executors
- ConexÃµes de banco
- ConfiguraÃ§Ãµes de email
- Plugins

### AutenticaÃ§Ã£o

Para produÃ§Ã£o, considere:
- Configurar autenticaÃ§Ã£o LDAP/OAuth
- Usar secrets managers
- Implementar HTTPS
- Configurar firewalls

## ğŸ“ˆ Monitoramento

### Logs

```bash
# Todos os logs
docker-compose logs -f

# Logs especÃ­ficos
docker-compose logs -f airflow-scheduler
docker-compose logs -f spark-master
```

### MÃ©tricas

- **Airflow**: Interface web com mÃ©tricas de DAGs
- **Spark**: Spark UI com mÃ©tricas de jobs
- **MinIO**: Console com estatÃ­sticas de storage
- **Jenkins**: Interface com histÃ³rico de builds

## ğŸš¨ Troubleshooting

### Problemas Comuns

1. **Porta jÃ¡ em uso**
   ```bash
   # Verificar portas ocupadas
   netstat -tulpn | grep :8080
   ```

2. **MemÃ³ria insuficiente**
   ```bash
   # Verificar uso de memÃ³ria
   docker stats
   ```

3. **PermissÃµes do Airflow**
   ```bash
   # Corrigir permissÃµes
   sudo chown -R 1000:0 airflow/
   ```

4. **Containers nÃ£o iniciam**
   ```bash
   # Ver logs detalhados
   docker-compose logs [serviÃ§o]
   ```

### RecuperaÃ§Ã£o

```bash
# Restart completo
./scripts/stop.sh
./scripts/start.sh

# Limpeza completa (CUIDADO: remove todos os dados)
make clean
make start
```

## ğŸ”§ Troubleshooting

### ğŸš¨ Problemas Comuns

#### 1. **Permission denied while trying to connect to Docker daemon**
```bash
# SoluÃ§Ã£o: Adicionar usuÃ¡rio ao grupo docker
sudo usermod -aG docker $USER

# Depois faÃ§a logout/login ou:
newgrp docker

# Verificar se funcionou:
groups | grep docker
```

#### 2. **Erro "Port already in use"**
```bash
# Verificar portas ocupadas
sudo netstat -tlnp | grep :8080

# Parar todos os containers
make stop-all

# Ou alterar portas no arquivo .env
nano .env
```

#### 3. **Containers nÃ£o iniciam (recursos insuficientes)**
```bash
# Verificar recursos disponÃ­veis
make requirements

# Usar ambiente mais leve
make minimal  # Em vez de make start
```

#### 4. **Airflow: "ModuleNotFoundError: No module named 'airflow'"**
```bash
# PROBLEMA: VersÃ£o antiga do Airflow em cache/volumes antigos
# SOLUÃ‡ÃƒO: Limpeza completa do Airflow

# OpÃ§Ã£o 1: Limpeza especÃ­fica do Airflow (recomendada)
make clean-airflow
make start

# OpÃ§Ã£o 2: Reset completo do ambiente
make reset-env
make start

# Verificar se funcionou
make health
```

#### 5. **Airflow: "Too old Airflow version" ou problemas de inicializaÃ§Ã£o**
```bash
# PROBLEMA: Containers/imagens antigas conflitando
# SOLUÃ‡ÃƒO: Limpeza e rebuild

# 1. Parar tudo
make stop-all

# 2. Limpar imagens antigas
docker images apache/airflow --format "table {{.Repository}}:{{.Tag}}\t{{.ID}}"
docker rmi $(docker images apache/airflow:2.7.0* -q) 2>/dev/null || true

# 3. Fazer pull das novas imagens
make pull

# 4. Iniciar novamente
make start
```

#### 6. **Airflow nÃ£o carrega DAGs**
```bash
# Verificar logs do Airflow
make logs-airflow

# Verificar permissÃµes da pasta dags
ls -la airflow/dags/

# Corrigir permissÃµes se necessÃ¡rio
sudo chown -R $USER:$USER airflow/
```

### ğŸ” Comandos de DiagnÃ³stico

```bash
# Status completo do sistema
make health               # Status de todos os serviÃ§os
make ps-all              # Containers de todos os ambientes
make check               # Verificar dependÃªncias
make info                # InformaÃ§Ãµes do ambiente

# Verificar recursos
make top                 # Uso de CPU/RAM
docker system df         # Uso de disco

# Logs especÃ­ficos
make logs-[serviÃ§o]      # Logs de serviÃ§o especÃ­fico
```

### ğŸ’¾ **Em Caso de Problemas Graves**

```bash
# Reset completo (CUIDADO: remove todos os dados!)
make stop-all
make clean

# Reinstalar do zero
cp .env.example .env
nano .env               # Configure suas variÃ¡veis
make minimal           # ComeÃ§ar com ambiente simples
```

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo LICENSE para detalhes.

## ğŸ†˜ Suporte

Para problemas ou dÃºvidas:
1. Verifique a seÃ§Ã£o de troubleshooting
2. Consulte os logs dos serviÃ§os
3. Abra uma issue no repositÃ³rio

## ğŸ”— Recursos Ãšteis

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [MinIO Documentation](https://docs.min.io/)
- [Jenkins Documentation](https://www.jenkins.io/doc/)
- [Jupyter Documentation](https://jupyter.readthedocs.io/)

---

**ğŸ‰ Aproveite seu ambiente BigData integrado!**