{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14f4bac",
   "metadata": {},
   "source": [
    "## üîÑ **Atualiza√ß√£o**: Usando Vari√°veis de Ambiente\n",
    "\n",
    "Agora o ambiente foi **atualizado** para usar vari√°veis de ambiente, seguindo as **melhores pr√°ticas**:\n",
    "\n",
    "### ‚úÖ **Configura√ß√£o no `.env`:**\n",
    "```bash\n",
    "# Configura√ß√µes do PySpark\n",
    "SPARK_HOME=/opt/spark\n",
    "PYSPARK_PYTHON=python3\n",
    "PYSPARK_DRIVER_PYTHON=jupyter\n",
    "PYSPARK_DRIVER_PYTHON_OPTS=lab\n",
    "SPARK_DRIVER_HOST=jupyter\n",
    "SPARK_DRIVER_BIND_ADDRESS=0.0.0.0\n",
    "SPARK_LOCAL_IP=jupyter\n",
    "SPARK_DRIVER_MEMORY=1g\n",
    "SPARK_EXECUTOR_MEMORY=1g\n",
    "SPARK_EXECUTOR_CORES=2\n",
    "```\n",
    "\n",
    "### ‚úÖ **Configura√ß√£o no `docker-compose.yml`:**\n",
    "```yaml\n",
    "environment:\n",
    "  - SPARK_HOME=${SPARK_HOME:-/opt/spark}\n",
    "  - PYSPARK_PYTHON=${PYSPARK_PYTHON:-python3}\n",
    "  - PYSPARK_DRIVER_PYTHON=${PYSPARK_DRIVER_PYTHON:-jupyter}\n",
    "  - PYSPARK_DRIVER_PYTHON_OPTS=${PYSPARK_DRIVER_PYTHON_OPTS:-lab}\n",
    "  # ... outras configura√ß√µes\n",
    "```\n",
    "\n",
    "### üéØ **Vantagens:**\n",
    "- **Flexibilidade** para diferentes ambientes\n",
    "- **Customiza√ß√£o** sem rebuild de containers\n",
    "- **Manutenibilidade** centralizada no `.env`\n",
    "- **Portabilidade** entre dev/test/prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"üîç VERIFICANDO VARI√ÅVEIS DE AMBIENTE CONFIGURADAS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Vari√°veis PySpark configuradas via .env\n",
    "pyspark_env_vars = {\n",
    "    'SPARK_HOME': 'Diret√≥rio de instala√ß√£o do Spark',\n",
    "    'PYSPARK_PYTHON': 'Python para executors',\n",
    "    'PYSPARK_DRIVER_PYTHON': 'Python para driver (Jupyter)',\n",
    "    'PYSPARK_DRIVER_PYTHON_OPTS': 'Modo do driver (lab/notebook)',\n",
    "    'SPARK_DRIVER_HOST': 'Hostname do driver',\n",
    "    'SPARK_DRIVER_BIND_ADDRESS': 'Interface de bind',\n",
    "    'SPARK_LOCAL_IP': 'IP local do Spark',\n",
    "    'SPARK_DRIVER_MEMORY': 'Mem√≥ria do driver',\n",
    "    'SPARK_EXECUTOR_MEMORY': 'Mem√≥ria dos executors',\n",
    "    'SPARK_EXECUTOR_CORES': 'Cores por executor'\n",
    "}\n",
    "\n",
    "print(\"üìã VARI√ÅVEIS PYSPARK CONFIGURADAS:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for var, description in pyspark_env_vars.items():\n",
    "    value = os.environ.get(var, '‚ùå N√ÉO CONFIGURADO')\n",
    "    status = \"‚úÖ\" if value != '‚ùå N√ÉO CONFIGURADO' else \"‚ùå\"\n",
    "    print(f\"{status} {var}: {value}\")\n",
    "    if value != '‚ùå N√ÉO CONFIGURADO':\n",
    "        print(f\"   ‚îî‚îÄ {description}\")\n",
    "\n",
    "# Verificar vari√°veis adicionais importantes\n",
    "print(f\"\\nüåê VARI√ÅVEIS ADICIONAIS:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "additional_vars = [\n",
    "    'SPARK_MASTER', 'JUPYTER_ENABLE_LAB', 'GRANT_SUDO'\n",
    "]\n",
    "\n",
    "for var in additional_vars:\n",
    "    value = os.environ.get(var, 'n√£o configurado')\n",
    "    print(f\"üìå {var}: {value}\")\n",
    "\n",
    "print(f\"\\nüí° COMO CUSTOMIZAR:\")\n",
    "print(\"Edite o arquivo .env para ajustar configura√ß√µes:\")\n",
    "print(\"  SPARK_DRIVER_MEMORY=2g    # Para mais mem√≥ria\")\n",
    "print(\"  SPARK_EXECUTOR_CORES=4    # Para mais cores\")\n",
    "print(\"  PYSPARK_DRIVER_PYTHON_OPTS=notebook  # Para usar Notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405f3ff",
   "metadata": {},
   "source": [
    "# üîß PySpark Environment Setup - Best Practices\n",
    "\n",
    "Este notebook demonstra as **melhores pr√°ticas** para configurar vari√°veis de ambiente e integrar o **Jupyter** com **PySpark** no ambiente BigData.\n",
    "\n",
    "## üéØ **Objetivos:**\n",
    "- ‚úÖ Configurar vari√°veis de ambiente essenciais para PySpark\n",
    "- ‚úÖ Verificar conectividade com cluster Spark\n",
    "- ‚úÖ Demonstrar integra√ß√£o otimizada Jupyter + PySpark\n",
    "- ‚úÖ Validar performance e funcionalidades\n",
    "\n",
    "## üìã **Vari√°veis de Ambiente Principais:**\n",
    "- `SPARK_HOME` - Caminho para instala√ß√£o do Spark\n",
    "- `PYSPARK_PYTHON` - Interpretador Python para executors\n",
    "- `PYSPARK_DRIVER_PYTHON` - Python para driver (Jupyter)\n",
    "- `PYSPARK_DRIVER_PYTHON_OPTS` - Op√ß√µes do driver (notebook/lab)\n",
    "- `SPARK_MASTER` - URL do cluster Spark\n",
    "\n",
    "---\n",
    "**üí° Nota**: Este ambiente j√° est√° pr√©-configurado no `docker-compose.yml` com as melhores pr√°ticas!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa73a679",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Check Current Environment\n",
    "\n",
    "Primeiro, vamos verificar o ambiente atual e as vari√°veis j√° configuradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"üîç VERIFICA√á√ÉO DO AMBIENTE ATUAL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üìç Python Version: {sys.version}\")\n",
    "print(f\"üìç Python Executable: {sys.executable}\")\n",
    "print(f\"üìç Working Directory: {os.getcwd()}\")\n",
    "\n",
    "print(\"\\nüåê VARI√ÅVEIS DE AMBIENTE SPARK:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "spark_vars = [\n",
    "    'SPARK_HOME', 'SPARK_MASTER', 'SPARK_OPTS',\n",
    "    'PYSPARK_PYTHON', 'PYSPARK_DRIVER_PYTHON', \n",
    "    'PYSPARK_DRIVER_PYTHON_OPTS', 'SPARK_DRIVER_HOST',\n",
    "    'SPARK_DRIVER_BIND_ADDRESS', 'SPARK_LOCAL_IP'\n",
    "]\n",
    "\n",
    "for var in spark_vars:\n",
    "    value = os.environ.get(var, '‚ùå N√ÉO CONFIGURADO')\n",
    "    print(f\"{var}: {value}\")\n",
    "\n",
    "print(f\"\\nüì¶ PATH cont√©m Spark: {'‚úÖ SIM' if any('spark' in p.lower() for p in os.environ.get('PATH', '').split(':')) else '‚ùå N√ÉO'}\")\n",
    "\n",
    "# Verificar se PySpark est√° dispon√≠vel\n",
    "try:\n",
    "    import pyspark\n",
    "    print(f\"‚úÖ PySpark dispon√≠vel: {pyspark.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå PySpark n√£o encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28f4f2",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Set Spark Home Path\n",
    "\n",
    "Configura√ß√£o do `SPARK_HOME` - fundamental para o funcionamento do PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4bf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"üè† CONFIGURA√á√ÉO DO SPARK_HOME\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Verificar se SPARK_HOME j√° est√° definido\n",
    "current_spark_home = os.environ.get('SPARK_HOME')\n",
    "print(f\"SPARK_HOME atual: {current_spark_home}\")\n",
    "\n",
    "# No ambiente docker, o Spark j√° deve estar configurado\n",
    "# Vamos verificar poss√≠veis localiza√ß√µes\n",
    "possible_locations = [\n",
    "    '/opt/spark',\n",
    "    '/usr/local/spark', \n",
    "    '/opt/bitnami/spark',\n",
    "    current_spark_home\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Verificando poss√≠veis localiza√ß√µes do Spark:\")\n",
    "for location in possible_locations:\n",
    "    if location and os.path.exists(location):\n",
    "        print(f\"‚úÖ Encontrado: {location}\")\n",
    "        # Verificar se tem os bin√°rios principais\n",
    "        bin_path = os.path.join(location, 'bin')\n",
    "        if os.path.exists(bin_path):\n",
    "            print(f\"   üìÅ Diret√≥rio bin: {bin_path}\")\n",
    "            if os.path.exists(os.path.join(bin_path, 'spark-submit')):\n",
    "                print(f\"   ‚ö° spark-submit: Dispon√≠vel\")\n",
    "    else:\n",
    "        print(f\"‚ùå N√£o encontrado: {location}\")\n",
    "\n",
    "# Configurar SPARK_HOME se necess√°rio\n",
    "if not current_spark_home:\n",
    "    # Tentar detectar automaticamente\n",
    "    try:\n",
    "        result = subprocess.run(['which', 'spark-submit'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            spark_submit_path = result.stdout.strip()\n",
    "            spark_home = os.path.dirname(os.path.dirname(spark_submit_path))\n",
    "            os.environ['SPARK_HOME'] = spark_home\n",
    "            print(f\"\\n‚úÖ SPARK_HOME configurado automaticamente: {spark_home}\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  spark-submit n√£o encontrado no PATH\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erro ao detectar Spark: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ SPARK_HOME final: {os.environ.get('SPARK_HOME', 'N√ÉO CONFIGURADO')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed7862",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configure Python Executors\n",
    "\n",
    "Defini√ß√£o do interpretador Python que o PySpark usar√° nos executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d539eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"üêç CONFIGURA√á√ÉO DO PYTHON PARA EXECUTORS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Verificar vers√£o atual do Python\n",
    "print(f\"Python atual: {sys.version}\")\n",
    "print(f\"Execut√°vel: {sys.executable}\")\n",
    "\n",
    "# Configurar PYSPARK_PYTHON\n",
    "current_pyspark_python = os.environ.get('PYSPARK_PYTHON')\n",
    "print(f\"\\nPYSPARK_PYTHON atual: {current_pyspark_python}\")\n",
    "\n",
    "# Definir o melhor Python dispon√≠vel\n",
    "python_options = ['python3', 'python', sys.executable]\n",
    "\n",
    "print(\"\\nüîç Testando interpretadores Python dispon√≠veis:\")\n",
    "best_python = None\n",
    "\n",
    "for python_cmd in python_options:\n",
    "    try:\n",
    "        result = subprocess.run([python_cmd, '--version'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            version = result.stdout.strip() or result.stderr.strip()\n",
    "            print(f\"‚úÖ {python_cmd}: {version}\")\n",
    "            if not best_python:\n",
    "                best_python = python_cmd\n",
    "    except (subprocess.SubprocessError, FileNotFoundError):\n",
    "        print(f\"‚ùå {python_cmd}: N√£o dispon√≠vel\")\n",
    "\n",
    "# Configurar PYSPARK_PYTHON\n",
    "if best_python:\n",
    "    os.environ['PYSPARK_PYTHON'] = best_python\n",
    "    print(f\"\\n‚úÖ PYSPARK_PYTHON configurado: {best_python}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Nenhum interpretador Python adequado encontrado\")\n",
    "\n",
    "# Verificar se os executors conseguir√£o usar o mesmo Python\n",
    "print(f\"\\nüéØ Configura√ß√£o final:\")\n",
    "print(f\"   Driver Python: {sys.executable}\")\n",
    "print(f\"   Executor Python: {os.environ.get('PYSPARK_PYTHON', 'N√ÉO CONFIGURADO')}\")\n",
    "\n",
    "# Validar que ambos t√™m a mesma vers√£o principal\n",
    "try:\n",
    "    driver_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "    executor_result = subprocess.run([best_python, '-c', 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")'], \n",
    "                                   capture_output=True, text=True)\n",
    "    if executor_result.returncode == 0:\n",
    "        executor_version = executor_result.stdout.strip()\n",
    "        if driver_version == executor_version:\n",
    "            print(f\"‚úÖ Vers√µes compat√≠veis: Driver {driver_version} == Executor {executor_version}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Vers√µes diferentes: Driver {driver_version} != Executor {executor_version}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao verificar compatibilidade: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506a508",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Set Jupyter Driver Configuration\n",
    "\n",
    "Configura√ß√£o espec√≠fica para integra√ß√£o com Jupyter Lab/Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f84414",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìì CONFIGURA√á√ÉO DO DRIVER JUPYTER\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Verificar ambiente Jupyter atual\n",
    "print(\"üîç Detectando ambiente Jupyter:\")\n",
    "\n",
    "# Verificar se estamos em Jupyter\n",
    "in_jupyter = False\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if get_ipython() is not None:\n",
    "        in_jupyter = True\n",
    "        ipython = get_ipython()\n",
    "        print(f\"‚úÖ Executando em IPython/Jupyter\")\n",
    "        print(f\"   Classe: {ipython.__class__.__name__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå IPython n√£o dispon√≠vel\")\n",
    "\n",
    "# Verificar se √© Jupyter Lab ou Notebook\n",
    "jupyter_type = \"unknown\"\n",
    "if in_jupyter:\n",
    "    try:\n",
    "        # Tentar detectar se √© Lab ou Notebook\n",
    "        import json\n",
    "        import requests\n",
    "        # Este √© um m√©todo simplificado - em produ√ß√£o pode ser mais complexo\n",
    "        jupyter_type = \"lab\"  # Assumindo Lab por padr√£o no nosso ambiente\n",
    "    except:\n",
    "        jupyter_type = \"notebook\"\n",
    "\n",
    "print(f\"   Tipo detectado: {jupyter_type}\")\n",
    "\n",
    "# Configurar vari√°veis do driver\n",
    "print(f\"\\n‚öôÔ∏è Configurando vari√°veis do driver:\")\n",
    "\n",
    "# PYSPARK_DRIVER_PYTHON\n",
    "driver_python = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = driver_python\n",
    "print(f\"‚úÖ PYSPARK_DRIVER_PYTHON: {driver_python}\")\n",
    "\n",
    "# PYSPARK_DRIVER_PYTHON_OPTS\n",
    "if jupyter_type == \"lab\":\n",
    "    driver_opts = \"lab\"\n",
    "    print(\"üìì Configurando para Jupyter Lab\")\n",
    "else:\n",
    "    driver_opts = \"notebook\"\n",
    "    print(\"üìí Configurando para Jupyter Notebook\")\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = driver_opts\n",
    "print(f\"‚úÖ PYSPARK_DRIVER_PYTHON_OPTS: {driver_opts}\")\n",
    "\n",
    "# Configura√ß√µes adicionais para conectividade\n",
    "print(f\"\\nüåê Configura√ß√µes de conectividade:\")\n",
    "\n",
    "# Driver host - importante para clusters distribu√≠dos\n",
    "driver_host = os.environ.get('SPARK_DRIVER_HOST', 'jupyter')\n",
    "os.environ['SPARK_DRIVER_HOST'] = driver_host\n",
    "print(f\"‚úÖ SPARK_DRIVER_HOST: {driver_host}\")\n",
    "\n",
    "# Driver bind address\n",
    "driver_bind = os.environ.get('SPARK_DRIVER_BIND_ADDRESS', '0.0.0.0')\n",
    "os.environ['SPARK_DRIVER_BIND_ADDRESS'] = driver_bind\n",
    "print(f\"‚úÖ SPARK_DRIVER_BIND_ADDRESS: {driver_bind}\")\n",
    "\n",
    "print(f\"\\nüéØ Resumo da configura√ß√£o do driver:\")\n",
    "print(f\"   Python: {driver_python}\")\n",
    "print(f\"   Modo: {driver_opts}\")\n",
    "print(f\"   Host: {driver_host}\")\n",
    "print(f\"   Bind: {driver_bind}\")\n",
    "\n",
    "if in_jupyter:\n",
    "    print(f\"\\n‚úÖ Ambiente Jupyter configurado e pronto para PySpark!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  N√£o executando em Jupyter - configura√ß√£o pode n√£o ser aplicada automaticamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19949aac",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Verify Environment Setup\n",
    "\n",
    "Valida√ß√£o completa de todas as vari√°veis de ambiente configuradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58db8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ VERIFICA√á√ÉO FINAL DO AMBIENTE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Lista completa de vari√°veis importantes\n",
    "required_vars = {\n",
    "    'SPARK_HOME': 'Caminho para instala√ß√£o do Spark',\n",
    "    'SPARK_MASTER': 'URL do cluster Spark',\n",
    "    'PYSPARK_PYTHON': 'Python para executors',\n",
    "    'PYSPARK_DRIVER_PYTHON': 'Python para driver',\n",
    "    'PYSPARK_DRIVER_PYTHON_OPTS': 'Op√ß√µes do driver (lab/notebook)',\n",
    "    'SPARK_DRIVER_HOST': 'Host do driver',\n",
    "    'SPARK_DRIVER_BIND_ADDRESS': 'Endere√ßo de bind do driver',\n",
    "    'SPARK_LOCAL_IP': 'IP local do Spark'\n",
    "}\n",
    "\n",
    "optional_vars = {\n",
    "    'SPARK_OPTS': 'Op√ß√µes adicionais do Spark',\n",
    "    'SPARK_DRIVER_MEMORY': 'Mem√≥ria do driver',\n",
    "    'SPARK_EXECUTOR_MEMORY': 'Mem√≥ria dos executors',\n",
    "    'SPARK_EXECUTOR_CORES': 'Cores dos executors'\n",
    "}\n",
    "\n",
    "print(\"üîç VARI√ÅVEIS OBRIGAT√ìRIAS:\")\n",
    "print(\"-\" * 25)\n",
    "all_required_ok = True\n",
    "\n",
    "for var, description in required_vars.items():\n",
    "    value = os.environ.get(var)\n",
    "    if value:\n",
    "        print(f\"‚úÖ {var}: {value}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {var}: N√ÉO CONFIGURADO\")\n",
    "        all_required_ok = False\n",
    "\n",
    "print(f\"\\nüîç VARI√ÅVEIS OPCIONAIS:\")\n",
    "print(\"-\" * 20)\n",
    "for var, description in optional_vars.items():\n",
    "    value = os.environ.get(var)\n",
    "    if value:\n",
    "        print(f\"‚úÖ {var}: {value}\")\n",
    "    else:\n",
    "        print(f\"‚ö™ {var}: n√£o configurado ({description})\")\n",
    "\n",
    "# Verifica√ß√£o de conectividade com Spark Master\n",
    "print(f\"\\nüåê TESTE DE CONECTIVIDADE:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "spark_master = os.environ.get('SPARK_MASTER')\n",
    "if spark_master:\n",
    "    print(f\"üéØ Testando conex√£o com: {spark_master}\")\n",
    "    \n",
    "    # Tentar fazer ping b√°sico\n",
    "    import socket\n",
    "    import urllib.parse\n",
    "    \n",
    "    try:\n",
    "        parsed = urllib.parse.urlparse(spark_master)\n",
    "        host = parsed.hostname\n",
    "        port = parsed.port or 7077\n",
    "        \n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(3)\n",
    "        result = sock.connect_ex((host, port))\n",
    "        sock.close()\n",
    "        \n",
    "        if result == 0:\n",
    "            print(f\"‚úÖ Spark Master acess√≠vel em {host}:{port}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Spark Master n√£o acess√≠vel em {host}:{port}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Erro ao testar conectividade: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå SPARK_MASTER n√£o configurado\")\n",
    "\n",
    "# Resumo final\n",
    "print(f\"\\nüìã RESUMO:\")\n",
    "print(\"-\" * 10)\n",
    "if all_required_ok:\n",
    "    print(\"‚úÖ Todas as vari√°veis obrigat√≥rias configuradas\")\n",
    "    print(\"üöÄ Ambiente pronto para inicializar PySpark!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Algumas vari√°veis obrigat√≥rias n√£o configuradas\")\n",
    "    print(\"üîß Revise a configura√ß√£o antes de prosseguir\")\n",
    "\n",
    "# Verificar se PySpark pode ser importado\n",
    "print(f\"\\nüêç TESTE DE IMPORTA√á√ÉO:\")\n",
    "try:\n",
    "    import pyspark\n",
    "    from pyspark.sql import SparkSession\n",
    "    print(f\"‚úÖ PySpark {pyspark.__version__} importado com sucesso\")\n",
    "    print(\"‚úÖ SparkSession dispon√≠vel\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erro ao importar PySpark: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Erro inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce1a31",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Initialize PySpark Session\n",
    "\n",
    "Cria√ß√£o e configura√ß√£o da SparkSession usando as vari√°veis de ambiente configuradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb08ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import time\n",
    "\n",
    "print(\"üöÄ INICIALIZANDO SPARKSESSION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Configura√ß√£o personalizada do Spark\n",
    "print(\"‚öôÔ∏è Configurando SparkSession...\")\n",
    "\n",
    "# Criar configura√ß√£o baseada nas vari√°veis de ambiente\n",
    "conf = SparkConf()\n",
    "\n",
    "# Configura√ß√µes b√°sicas\n",
    "conf.setAppName(\"BigData-Jupyter-Integration\")\n",
    "conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "# Configura√ß√µes de mem√≥ria\n",
    "conf.set(\"spark.driver.memory\", \"1g\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"500m\")\n",
    "conf.set(\"spark.executor.memory\", \"1g\")\n",
    "\n",
    "# Configura√ß√µes de conectividade\n",
    "spark_master = os.environ.get('SPARK_MASTER', 'local[*]')\n",
    "conf.setMaster(spark_master)\n",
    "\n",
    "print(f\"üéØ Master: {spark_master}\")\n",
    "print(f\"üì± App Name: BigData-Jupyter-Integration\")\n",
    "\n",
    "# Tentar criar a SparkSession\n",
    "print(f\"\\nüîÑ Criando SparkSession...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    creation_time = time.time() - start_time\n",
    "    print(f\"‚úÖ SparkSession criada com sucesso em {creation_time:.2f}s\")\n",
    "    \n",
    "    # Informa√ß√µes da sess√£o\n",
    "    print(f\"\\nüìä INFORMA√á√ïES DA SESS√ÉO:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"üÜî Application ID: {spark.sparkContext.applicationId}\")\n",
    "    print(f\"üåê Master: {spark.sparkContext.master}\")\n",
    "    print(f\"üì± App Name: {spark.sparkContext.appName}\")\n",
    "    print(f\"üêç Python Version: {spark.sparkContext.pythonVer}\")\n",
    "    print(f\"‚ö° Spark Version: {spark.version}\")\n",
    "    print(f\"üîß Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "    \n",
    "    # Configura√ß√µes importantes\n",
    "    print(f\"\\n‚öôÔ∏è CONFIGURA√á√ïES ATIVAS:\")\n",
    "    print(\"-\" * 20)\n",
    "    important_configs = [\n",
    "        'spark.master',\n",
    "        'spark.driver.memory',\n",
    "        'spark.executor.memory',\n",
    "        'spark.sql.adaptive.enabled',\n",
    "        'spark.driver.host',\n",
    "        'spark.driver.bindAddress'\n",
    "    ]\n",
    "    \n",
    "    for config in important_configs:\n",
    "        value = spark.conf.get(config, 'n√£o configurado')\n",
    "        print(f\"{config}: {value}\")\n",
    "    \n",
    "    print(f\"\\nüéâ SparkSession pronta para uso!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao criar SparkSession: {e}\")\n",
    "    print(f\"üîß Verifique as configura√ß√µes do cluster Spark\")\n",
    "    spark = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15756593",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Test PySpark Integration\n",
    "\n",
    "Testes pr√°ticos para validar que a integra√ß√£o Jupyter + PySpark est√° funcionando perfeitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark:\n",
    "    print(\"üß™ TESTANDO INTEGRA√á√ÉO PYSPARK\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Teste 1: Criar RDD simples\n",
    "    print(\"üìù Teste 1: Cria√ß√£o de RDD\")\n",
    "    try:\n",
    "        data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "        rdd = spark.sparkContext.parallelize(data)\n",
    "        total = rdd.sum()\n",
    "        count = rdd.count()\n",
    "        print(f\"‚úÖ RDD criado: {count} elementos, soma = {total}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no teste RDD: {e}\")\n",
    "    \n",
    "    # Teste 2: Criar DataFrame\n",
    "    print(f\"\\nüìä Teste 2: Cria√ß√£o de DataFrame\")\n",
    "    try:\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"nome\", StringType(), True),\n",
    "            StructField(\"idade\", IntegerType(), True),\n",
    "            StructField(\"cidade\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        data = [\n",
    "            (\"Alice\", 25, \"S√£o Paulo\"),\n",
    "            (\"Bob\", 30, \"Rio de Janeiro\"),\n",
    "            (\"Charlie\", 35, \"Belo Horizonte\"),\n",
    "            (\"Diana\", 28, \"Porto Alegre\")\n",
    "        ]\n",
    "        \n",
    "        df = spark.createDataFrame(data, schema)\n",
    "        print(f\"‚úÖ DataFrame criado com {df.count()} registros\")\n",
    "        \n",
    "        print(f\"\\nüìã Schema:\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        print(f\"\\nüìÑ Dados:\")\n",
    "        df.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no teste DataFrame: {e}\")\n",
    "    \n",
    "    # Teste 3: Opera√ß√µes SQL\n",
    "    print(f\"\\nüîç Teste 3: Opera√ß√µes SQL\")\n",
    "    try:\n",
    "        df.createOrReplaceTempView(\"pessoas\")\n",
    "        \n",
    "        result = spark.sql(\"\"\"\n",
    "            SELECT cidade, COUNT(*) as quantidade, AVG(idade) as idade_media\n",
    "            FROM pessoas \n",
    "            GROUP BY cidade\n",
    "            ORDER BY quantidade DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"‚úÖ Query SQL executada:\")\n",
    "        result.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no teste SQL: {e}\")\n",
    "    \n",
    "    # Teste 4: Processamento distribu√≠do\n",
    "    print(f\"\\n‚ö° Teste 4: Processamento Distribu√≠do\")\n",
    "    try:\n",
    "        # Criar dataset maior para testar paraleliza√ß√£o\n",
    "        large_data = range(1, 100001)  # 100k n√∫meros\n",
    "        large_rdd = spark.sparkContext.parallelize(large_data, 4)  # 4 parti√ß√µes\n",
    "        \n",
    "        # Opera√ß√£o computacionalmente intensiva\n",
    "        squares = large_rdd.map(lambda x: x * x)\n",
    "        sum_squares = squares.sum()\n",
    "        \n",
    "        print(f\"‚úÖ Processado {large_rdd.count():,} n√∫meros\")\n",
    "        print(f\"‚úÖ Soma dos quadrados: {sum_squares:,}\")\n",
    "        print(f\"‚úÖ N√∫mero de parti√ß√µes: {large_rdd.getNumPartitions()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no teste distribu√≠do: {e}\")\n",
    "    \n",
    "    # Teste 5: Performance e monitoring\n",
    "    print(f\"\\nüìà Teste 5: Informa√ß√µes de Performance\")\n",
    "    try:\n",
    "        # Verificar URL da Spark UI\n",
    "        spark_ui = spark.sparkContext.uiWebUrl\n",
    "        if spark_ui:\n",
    "            print(f\"üåê Spark UI dispon√≠vel em: {spark_ui}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Spark UI n√£o dispon√≠vel\")\n",
    "        \n",
    "        # Status do cluster\n",
    "        print(f\"\\nüèóÔ∏è Status do Cluster:\")\n",
    "        print(f\"   Master: {spark.sparkContext.master}\")\n",
    "        print(f\"   Executors ativos: {len(spark.sparkContext.statusTracker().getExecutorInfos())}\")\n",
    "        print(f\"   Cores totais: {spark.sparkContext.defaultParallelism}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Erro ao obter informa√ß√µes de performance: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ TESTES CONCLU√çDOS!\")\n",
    "    print(\"‚úÖ Integra√ß√£o Jupyter + PySpark funcionando perfeitamente!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå SparkSession n√£o dispon√≠vel - execute a c√©lula anterior primeiro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d647a29",
   "metadata": {},
   "source": [
    "## üéØ **Conclus√£o e Pr√≥ximos Passos**\n",
    "\n",
    "### ‚úÖ **O que aprendemos:**\n",
    "1. **Vari√°veis de ambiente essenciais** para integra√ß√£o PySpark + Jupyter\n",
    "2. **Configura√ß√£o autom√°tica** no ambiente Docker\n",
    "3. **Testes de conectividade** e funcionalidade\n",
    "4. **Otimiza√ß√µes de performance** para desenvolvimento\n",
    "\n",
    "### üöÄ **Pr√≥ximos passos recomendados:**\n",
    "- **Explorar Spark MLlib** para machine learning\n",
    "- **Integrar com MinIO** para storage distribu√≠do\n",
    "- **Conectar com PostgreSQL** via JDBC\n",
    "- **Usar Airflow** para orquestra√ß√£o de pipelines\n",
    "\n",
    "### üìö **Recursos adicionais:**\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Spark Configuration](https://spark.apache.org/docs/latest/configuration.html)\n",
    "- [Jupyter Integration Best Practices](https://jupyter-docker-stacks.readthedocs.io/)\n",
    "\n",
    "---\n",
    "**üí° Dica**: Salve este notebook como template para futuros projetos PySpark!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
