{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5855e3b",
   "metadata": {},
   "source": [
    "# BigData Environment - Exemplo de Integra√ß√£o\n",
    "\n",
    "Este notebook demonstra como usar os servi√ßos integrados do ambiente BigData:\n",
    "- **Spark**: Processamento de dados distribu√≠do\n",
    "- **MinIO**: Storage de objetos\n",
    "- **Airflow**: Orquestra√ß√£o de workflows\n",
    "- **Jenkins**: CI/CD\n",
    "\n",
    "## 1. Configura√ß√£o Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d2eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar pacotes necess√°rios se n√£o estiverem dispon√≠veis\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "required_packages = ['minio', 'boto3']\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Instalando {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Importar bibliotecas necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, max, min\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from minio import Minio\n",
    "import io\n",
    "import json\n",
    "\n",
    "# Configurar visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62103db",
   "metadata": {},
   "source": [
    "## üîß Verifica√ß√£o do Ambiente\n",
    "\n",
    "**Se voc√™ encontrar o erro `ModuleNotFoundError: No module named 'pyspark'`, execute a c√©lula abaixo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Verifica√ß√£o e Instala√ß√£o do PySpark (se necess√°rio)\n",
    "try:\n",
    "    import pyspark\n",
    "    print(f\"‚úÖ PySpark j√° est√° instalado! Vers√£o: {pyspark.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  PySpark n√£o encontrado. Instalando...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    # Instalar PySpark\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyspark==3.5.0\"])\n",
    "    \n",
    "    # Tentar importar novamente\n",
    "    import pyspark\n",
    "    print(f\"‚úÖ PySpark instalado com sucesso! Vers√£o: {pyspark.__version__}\")\n",
    "\n",
    "# Testar importa√ß√µes b√°sicas\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, count, avg, max, min\n",
    "    print(\"‚úÖ M√≥dulos do PySpark importados com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao importar m√≥dulos do PySpark: {e}\")\n",
    "    print(\"üí° Reinicie o kernel e tente novamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31e426",
   "metadata": {},
   "source": [
    "## 2. Conex√£o com Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b0a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar sess√£o Spark conectada ao cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Jupyter-Spark-Integration\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Session criada: {spark.sparkContext.appName}\")\n",
    "print(f\"Spark UI: http://localhost:8081\")\n",
    "print(f\"Vers√£o do Spark: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a16758",
   "metadata": {},
   "source": [
    "## 3. Conex√£o com MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar cliente MinIO\n",
    "minio_client = Minio(\n",
    "    \"minio:9000\",\n",
    "    access_key=\"minioadmin\",\n",
    "    secret_key=\"minioadmin123\",\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "# Listar buckets dispon√≠veis\n",
    "try:\n",
    "    buckets = minio_client.list_buckets()\n",
    "    print(\"Buckets dispon√≠veis no MinIO:\")\n",
    "    for bucket in buckets:\n",
    "        print(f\"  - {bucket.name} (criado em: {bucket.creation_date})\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao conectar com MinIO: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674e33f",
   "metadata": {},
   "source": [
    "## 4. Cria√ß√£o e Processamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa92016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dataset de exemplo\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = {\n",
    "    'id': range(1, n_samples + 1),\n",
    "    'nome': [f'Usuario_{i}' for i in range(1, n_samples + 1)],\n",
    "    'idade': np.random.randint(18, 70, n_samples),\n",
    "    'departamento': np.random.choice(['TI', 'Vendas', 'RH', 'Marketing', 'Financeiro'], n_samples),\n",
    "    'salario': np.random.normal(5000, 1500, n_samples).round(2),\n",
    "    'experiencia': np.random.randint(0, 20, n_samples),\n",
    "    'satisfacao': np.random.uniform(1, 5, n_samples).round(1)\n",
    "}\n",
    "\n",
    "# Criar DataFrame Pandas\n",
    "df_pandas = pd.DataFrame(data)\n",
    "print(\"Dataset criado com sucesso!\")\n",
    "print(f\"Shape: {df_pandas.shape}\")\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d4eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter para Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "print(\"Esquema do DataFrame Spark:\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "print(\"\\nPrimeiras 10 linhas:\")\n",
    "df_spark.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09193d4d",
   "metadata": {},
   "source": [
    "## 5. An√°lise de Dados com Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001465d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas b√°sicas\n",
    "print(\"=== Estat√≠sticas B√°sicas ===\")\n",
    "df_spark.describe().show()\n",
    "\n",
    "# An√°lise por departamento\n",
    "print(\"\\n=== An√°lise por Departamento ===\")\n",
    "dept_stats = df_spark.groupBy(\"departamento\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_funcionarios\"),\n",
    "        avg(\"idade\").alias(\"idade_media\"),\n",
    "        avg(\"salario\").alias(\"salario_medio\"),\n",
    "        avg(\"experiencia\").alias(\"experiencia_media\"),\n",
    "        avg(\"satisfacao\").alias(\"satisfacao_media\")\n",
    "    ) \\\n",
    "    .orderBy(\"salario_medio\", ascending=False)\n",
    "\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e006ddc",
   "metadata": {},
   "source": [
    "## 6. Salvando Dados no MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar dados originais no MinIO via Spark\n",
    "try:\n",
    "    print(\"Salvando dados no MinIO...\")\n",
    "    \n",
    "    # Salvar dados originais\n",
    "    df_spark.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(\"s3a://jupyter-data/datasets/funcionarios\")\n",
    "    \n",
    "    # Salvar estat√≠sticas por departamento\n",
    "    dept_stats.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(\"s3a://jupyter-data/datasets/estatisticas_departamento\")\n",
    "    \n",
    "    print(\"Dados salvos com sucesso no MinIO!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao salvar no MinIO via Spark: {e}\")\n",
    "    print(\"Tentando salvar via cliente MinIO...\")\n",
    "    \n",
    "    # Fallback: salvar via cliente MinIO\n",
    "    csv_buffer = io.StringIO()\n",
    "    df_pandas.to_csv(csv_buffer, index=False)\n",
    "    csv_data = csv_buffer.getvalue().encode('utf-8')\n",
    "    \n",
    "    minio_client.put_object(\n",
    "        \"jupyter-data\",\n",
    "        \"datasets/funcionarios.csv\",\n",
    "        io.BytesIO(csv_data),\n",
    "        len(csv_data),\n",
    "        content_type=\"text/csv\"\n",
    "    )\n",
    "    print(\"Dados salvos via cliente MinIO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f690dcb6",
   "metadata": {},
   "source": [
    "## 7. Visualiza√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda015eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter estat√≠sticas para Pandas para visualiza√ß√£o\n",
    "dept_stats_pandas = dept_stats.toPandas()\n",
    "\n",
    "# Criar visualiza√ß√µes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('An√°lise de Funcion√°rios por Departamento', fontsize=16)\n",
    "\n",
    "# Gr√°fico 1: N√∫mero de funcion√°rios por departamento\n",
    "axes[0, 0].bar(dept_stats_pandas['departamento'], dept_stats_pandas['total_funcionarios'])\n",
    "axes[0, 0].set_title('Funcion√°rios por Departamento')\n",
    "axes[0, 0].set_xlabel('Departamento')\n",
    "axes[0, 0].set_ylabel('N√∫mero de Funcion√°rios')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 2: Sal√°rio m√©dio por departamento\n",
    "axes[0, 1].bar(dept_stats_pandas['departamento'], dept_stats_pandas['salario_medio'])\n",
    "axes[0, 1].set_title('Sal√°rio M√©dio por Departamento')\n",
    "axes[0, 1].set_xlabel('Departamento')\n",
    "axes[0, 1].set_ylabel('Sal√°rio M√©dio (R$)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 3: Idade m√©dia por departamento\n",
    "axes[1, 0].bar(dept_stats_pandas['departamento'], dept_stats_pandas['idade_media'])\n",
    "axes[1, 0].set_title('Idade M√©dia por Departamento')\n",
    "axes[1, 0].set_xlabel('Departamento')\n",
    "axes[1, 0].set_ylabel('Idade M√©dia (anos)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 4: Satisfa√ß√£o m√©dia por departamento\n",
    "axes[1, 1].bar(dept_stats_pandas['departamento'], dept_stats_pandas['satisfacao_media'])\n",
    "axes[1, 1].set_title('Satisfa√ß√£o M√©dia por Departamento')\n",
    "axes[1, 1].set_xlabel('Departamento')\n",
    "axes[1, 1].set_ylabel('Satisfa√ß√£o M√©dia (1-5)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207deaf9",
   "metadata": {},
   "source": [
    "## 8. An√°lise de Correla√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de correla√ß√£o\n",
    "numeric_columns = ['idade', 'salario', 'experiencia', 'satisfacao']\n",
    "correlation_matrix = df_pandas[numeric_columns].corr()\n",
    "\n",
    "# Heatmap de correla√ß√£o\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Matriz de Correla√ß√£o')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot: Experi√™ncia vs Sal√°rio\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(df_pandas['experiencia'], df_pandas['salario'], \n",
    "                     c=df_pandas['satisfacao'], cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(scatter, label='Satisfa√ß√£o')\n",
    "plt.xlabel('Experi√™ncia (anos)')\n",
    "plt.ylabel('Sal√°rio (R$)')\n",
    "plt.title('Rela√ß√£o entre Experi√™ncia, Sal√°rio e Satisfa√ß√£o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad0db1",
   "metadata": {},
   "source": [
    "## 9. Limpeza e Finaliza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d66865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizar sess√£o Spark\n",
    "spark.stop()\n",
    "print(\"Sess√£o Spark finalizada.\")\n",
    "\n",
    "print(\"\\n=== Resumo da Execu√ß√£o ===\")\n",
    "print(f\"‚úÖ Dados processados: {len(df_pandas)} registros\")\n",
    "print(f\"‚úÖ Departamentos analisados: {df_pandas['departamento'].nunique()}\")\n",
    "print(f\"‚úÖ Visualiza√ß√µes criadas: 6 gr√°ficos\")\n",
    "print(f\"‚úÖ Dados salvos no MinIO\")\n",
    "print(f\"\\nüîó Acesse os outros servi√ßos:\")\n",
    "print(f\"   ‚Ä¢ Airflow: http://localhost:8080 (admin/admin)\")\n",
    "print(f\"   ‚Ä¢ Spark UI: http://localhost:8081\")\n",
    "print(f\"   ‚Ä¢ MinIO: http://localhost:9001 (minioadmin/minioadmin123)\")\n",
    "print(f\"   ‚Ä¢ Jenkins: http://localhost:8082 (admin/admin)\")\n",
    "print(f\"   ‚Ä¢ Flower (Celery): http://localhost:5555\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}