# ‚ö° PySpark - Configura√ß√£o de Vari√°veis de Ambiente

Guia completo para configura√ß√£o de vari√°veis de ambiente do PySpark no ambiente BigData.

## üéØ Objetivo

Este documento descreve as melhores pr√°ticas para configura√ß√£o de vari√°veis de ambiente PySpark, garantindo integra√ß√£o adequada entre Jupyter, Spark e MinIO.

## üîß Vari√°veis Essenciais

### Configura√ß√£o Base
```bash
# Localiza√ß√£o do Spark
SPARK_HOME=/usr/local/spark

# Python paths para PySpark
PYTHONPATH=/usr/local/spark/python:/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip

# Configura√ß√µes Python do PySpark
PYSPARK_PYTHON=python3
PYSPARK_DRIVER_PYTHON=jupyter
PYSPARK_DRIVER_PYTHON_OPTS=lab
```

### Configura√ß√£o de Recursos
```bash
# Mem√≥ria
SPARK_DRIVER_MEMORY=1g
SPARK_EXECUTOR_MEMORY=1g

# CPU Cores
SPARK_DRIVER_CORES=1
SPARK_EXECUTOR_CORES=2

# Inst√¢ncias
SPARK_EXECUTOR_INSTANCES=2
```

### Configura√ß√£o de Rede
```bash
# Cluster Spark
SPARK_MASTER=spark://spark-master:7077

# Configura√ß√µes de rede
SPARK_DRIVER_HOST=jupyter
SPARK_DRIVER_BIND_ADDRESS=0.0.0.0
SPARK_LOCAL_IP=jupyter
```

## üìä Configura√ß√£o por Ambiente

### Ambiente Principal (docker-compose.yml)
```yaml
environment:
  - SPARK_HOME=/usr/local/spark
  - PYTHONPATH=/usr/local/spark/python:/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip
  - GRANT_SUDO=yes
  - SPARK_MASTER=${SPARK_MASTER:-spark://spark-master:7077}
  - PYSPARK_PYTHON=${PYSPARK_PYTHON:-python3}
  - PYSPARK_DRIVER_PYTHON=${PYSPARK_DRIVER_PYTHON:-jupyter}
  - PYSPARK_DRIVER_PYTHON_OPTS=${PYSPARK_DRIVER_PYTHON_OPTS:-lab}
  - SPARK_DRIVER_MEMORY=${SPARK_DRIVER_MEMORY:-1g}
  - SPARK_EXECUTOR_MEMORY=${SPARK_EXECUTOR_MEMORY:-1g}
  - SPARK_DRIVER_CORES=${SPARK_DRIVER_CORES:-1}
  - SPARK_EXECUTOR_CORES=${SPARK_EXECUTOR_CORES:-2}
  - SPARK_EXECUTOR_INSTANCES=${SPARK_EXECUTOR_INSTANCES:-2}
```

### Ambiente Lab (docker-compose.lab.yml)
```yaml
environment:
  - SPARK_DRIVER_MEMORY=${SPARK_DRIVER_MEMORY:-512m}
  - SPARK_EXECUTOR_MEMORY=${SPARK_EXECUTOR_MEMORY:-512m}
  # ... outras configura√ß√µes similares com recursos reduzidos
```

### Ambiente Minimal (docker-compose.minimal.yml)
```yaml
environment:
  - SPARK_DRIVER_MEMORY=${SPARK_DRIVER_MEMORY:-512m}
  - SPARK_EXECUTOR_MEMORY=${SPARK_EXECUTOR_MEMORY:-512m}
  # ... configura√ß√µes para Spark local
```

## üîç Resolu√ß√£o de Problemas

### ModuleNotFoundError: No module named 'pyspark'

**Causa**: PYTHONPATH n√£o configurado corretamente

**Solu√ß√£o**:
```bash
# Verificar se PYTHONPATH est√° definido
echo $PYTHONPATH

# Deve incluir:
# /usr/local/spark/python:/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip
```

### Erro de Conex√£o com Spark Master

**Causa**: SPARK_MASTER n√£o acess√≠vel

**Solu√ß√£o**:
```python
# Verificar conectividade
import socket
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
result = sock.connect_ex(('spark-master', 7077))
if result == 0:
    print("Conex√£o OK")
else:
    print("Erro de conex√£o")
```

### Problemas de Mem√≥ria

**Causa**: Configura√ß√µes inadequadas de mem√≥ria

**Solu√ß√£o**:
```bash
# Ajustar no .env
SPARK_DRIVER_MEMORY=2g
SPARK_EXECUTOR_MEMORY=2g
```

## üõ†Ô∏è Arquivo .env

Exemplo de configura√ß√£o no arquivo `.env`:

```bash
# PySpark Configuration
SPARK_HOME=/opt/spark
PYSPARK_PYTHON=python3
PYSPARK_DRIVER_PYTHON=jupyter
PYSPARK_DRIVER_PYTHON_OPTS=lab
SPARK_DRIVER_HOST=jupyter
SPARK_DRIVER_BIND_ADDRESS=0.0.0.0
SPARK_LOCAL_IP=jupyter
SPARK_DRIVER_MEMORY=1g
SPARK_EXECUTOR_MEMORY=1g
SPARK_EXECUTOR_CORES=2
SPARK_DRIVER_CORES=1
SPARK_EXECUTOR_INSTANCES=2
SPARK_MASTER=spark://spark-master:7077
```

## üìù Valida√ß√£o da Configura√ß√£o

### Teste B√°sico
```python
import pyspark
print(f"PySpark version: {pyspark.__version__}")

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Test").getOrCreate()
print(f"Spark context: {spark.sparkContext.appName}")
```

### Teste de Integra√ß√£o
```python
# Teste completo de integra√ß√£o
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Integration-Test") \
    .master("spark://spark-master:7077") \
    .config("spark.executor.memory", "1g") \
    .config("spark.executor.cores", "1") \
    .getOrCreate()

# Criar DataFrame teste
data = [("Alice", 25), ("Bob", 30)]
df = spark.createDataFrame(data, ["name", "age"])
df.show()

spark.stop()
```

## üîó Refer√™ncias

- [Documenta√ß√£o PySpark](https://spark.apache.org/docs/latest/api/python/)
- [Configura√ß√£o Spark](https://spark.apache.org/docs/latest/configuration.html)
- [Jupyter + PySpark](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html#jupyter-pyspark-notebook)