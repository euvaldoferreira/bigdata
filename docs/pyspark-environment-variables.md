# üîß PySpark Environment Variables - Best Practices

## üìã **Por que usar vari√°veis de ambiente?**

### ‚úÖ **Vantagens:**
1. **Flexibilidade** - Configura√ß√µes diferentes para dev/test/prod
2. **Reutiliza√ß√£o** - Mesmo c√≥digo, configura√ß√µes diferentes
3. **Seguran√ßa** - Credenciais e configura√ß√µes sens√≠veis separadas
4. **Manutenibilidade** - Mudan√ßas sem rebuild de containers
5. **Portabilidade** - Facilita deploy em diferentes ambientes

## üåê **Vari√°veis PySpark Configuradas**

### üìç **Caminhos e Execut√°veis:**
```bash
SPARK_HOME=/opt/spark                    # Diret√≥rio de instala√ß√£o do Spark
PYSPARK_PYTHON=python3                   # Python para executors
PYSPARK_DRIVER_PYTHON=jupyter            # Python para driver (Jupyter)
PYSPARK_DRIVER_PYTHON_OPTS=lab           # Modo Jupyter (lab/notebook)
```

### üåê **Conectividade:**
```bash
SPARK_DRIVER_HOST=jupyter                # Hostname do driver
SPARK_DRIVER_BIND_ADDRESS=0.0.0.0       # Interface de bind
SPARK_LOCAL_IP=jupyter                   # IP local para comunica√ß√£o
```

### ‚ö° **Performance:**
```bash
SPARK_DRIVER_MEMORY=1g                   # Mem√≥ria do driver
SPARK_EXECUTOR_MEMORY=1g                 # Mem√≥ria dos executors
SPARK_EXECUTOR_CORES=2                   # Cores por executor
```

## üîß **Como Customizar**

### 1Ô∏è‚É£ **Editar arquivo `.env`:**
```bash
# Para mais mem√≥ria em desenvolvimento:
SPARK_DRIVER_MEMORY=2g
SPARK_EXECUTOR_MEMORY=2g

# Para produ√ß√£o com mais cores:
SPARK_EXECUTOR_CORES=4
```

### 2Ô∏è‚É£ **Override via command line:**
```bash
SPARK_DRIVER_MEMORY=4g make start
```

### 3Ô∏è‚É£ **Diferentes ambientes:**
```bash
# .env.dev
SPARK_DRIVER_MEMORY=1g
SPARK_EXECUTOR_MEMORY=1g

# .env.prod  
SPARK_DRIVER_MEMORY=4g
SPARK_EXECUTOR_MEMORY=8g
```

## üìö **Documenta√ß√£o das Vari√°veis**

| Vari√°vel | Descri√ß√£o | Default | Personaliza√ß√£o |
|----------|-----------|---------|----------------|
| `SPARK_HOME` | Diret√≥rio do Spark | `/opt/spark` | Alterar apenas se Spark estiver em local diferente |
| `PYSPARK_PYTHON` | Python para executors | `python3` | `python`, `python3.8`, etc. |
| `PYSPARK_DRIVER_PYTHON` | Python para driver | `jupyter` | `ipython`, `python` |
| `PYSPARK_DRIVER_PYTHON_OPTS` | Modo Jupyter | `lab` | `notebook`, `--no-browser` |
| `SPARK_DRIVER_HOST` | Host do driver | `jupyter` | IP do container Jupyter |
| `SPARK_DRIVER_BIND_ADDRESS` | Interface bind | `0.0.0.0` | IP espec√≠fico se necess√°rio |
| `SPARK_LOCAL_IP` | IP local | `jupyter` | Para clusters multi-node |
| `SPARK_DRIVER_MEMORY` | Mem√≥ria driver | `1g` | `2g`, `4g`, etc. |
| `SPARK_EXECUTOR_MEMORY` | Mem√≥ria executors | `1g` | Baseado na mem√≥ria dispon√≠vel |
| `SPARK_EXECUTOR_CORES` | Cores por executor | `2` | Baseado em CPU dispon√≠vel |

## üöÄ **Exemplos de Configura√ß√£o**

### üî¨ **Desenvolvimento (recursos limitados):**
```bash
SPARK_DRIVER_MEMORY=512m
SPARK_EXECUTOR_MEMORY=512m
SPARK_EXECUTOR_CORES=1
```

### üè¢ **Produ√ß√£o (alta performance):**
```bash
SPARK_DRIVER_MEMORY=4g
SPARK_EXECUTOR_MEMORY=8g
SPARK_EXECUTOR_CORES=4
```

### üß™ **Teste (modo local):**
```bash
PYSPARK_DRIVER_PYTHON=python
PYSPARK_DRIVER_PYTHON_OPTS=
SPARK_MASTER=local[2]
```

## üîÑ **Implementa√ß√£o no docker-compose.yml**

```yaml
environment:
  # ‚úÖ CORRETO - Usando vari√°veis de ambiente
  - SPARK_HOME=${SPARK_HOME:-/opt/spark}
  - PYSPARK_PYTHON=${PYSPARK_PYTHON:-python3}
  
  # ‚ùå INCORRETO - Valores hardcoded
  - SPARK_HOME=/opt/spark
  - PYSPARK_PYTHON=python3
```

### **Benef√≠cios da sintaxe `${VAR:-default}`:**
- **Flexibilidade**: Usa valor do `.env` se definido
- **Fallback**: Usa valor padr√£o se vari√°vel n√£o existir
- **Portabilidade**: Funciona em qualquer ambiente

## üéØ **Pr√≥ximos Passos**

1. **Teste as configura√ß√µes** executando o notebook de exemplo
2. **Monitore performance** via Spark UI
3. **Ajuste mem√≥ria/cores** conforme necess√°rio
4. **Documente** configura√ß√µes espec√≠ficas do seu projeto

---

**üí° Dica**: Sempre teste configura√ß√µes em ambiente de desenvolvimento antes de aplicar em produ√ß√£o!