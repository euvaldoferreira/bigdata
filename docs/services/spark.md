# âš¡ Apache Spark

Apache Spark Ã© um framework de processamento distribuÃ­do para anÃ¡lise de big data.

## ğŸš€ CaracterÃ­sticas

- **Processamento RÃ¡pido**: Engine de anÃ¡lise unificado para big data
- **APIs MÃºltiplas**: SQL, DataFrames, Datasets, RDDs
- **Machine Learning**: MLlib para algoritmos de ML
- **Stream Processing**: Spark Streaming para dados em tempo real

## ğŸ”§ ConfiguraÃ§Ã£o no Projeto

### Cluster Spark
- **Spark Master**: http://localhost:8081
- **Workers**: ConfigurÃ¡veis via docker-compose
- **Modo**: Cluster standalone

### Estrutura de Arquivos
```
spark/
â”œâ”€â”€ apps/           # AplicaÃ§Ãµes Spark
â”œâ”€â”€ data/          # Dados de entrada/saÃ­da
â””â”€â”€ conf/          # ConfiguraÃ§Ãµes customizadas
```

## ğŸ“Š Uso BÃ¡sico

### PySpark no Jupyter
```python
from pyspark.sql import SparkSession

# Criar sessÃ£o Spark
spark = SparkSession.builder \
    .appName("MeuApp") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

# Ler dados
df = spark.read.csv("path/to/file.csv", header=True)

# Processamento
result = df.groupBy("coluna").count()
result.show()
```

### Submit de Jobs
```bash
# Via make command
make spark-submit APP=meu_app.py

# Via docker
docker exec spark-master spark-submit \
    --master spark://spark-master:7077 \
    /opt/bitnami/spark/apps/meu_app.py
```

## ğŸ”— IntegraÃ§Ã£o com Outros ServiÃ§os

- **Jupyter**: Notebooks interativos com PySpark
- **MinIO**: Storage distribuÃ­do via S3A
- **Airflow**: OrquestraÃ§Ã£o de jobs Spark

## ğŸ“š DocumentaÃ§Ã£o Oficial

- [DocumentaÃ§Ã£o Spark](https://spark.apache.org/docs/latest/)
- [PySpark API](https://spark.apache.org/docs/latest/api/python/)